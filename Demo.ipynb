{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependency libraries used for the SRG:\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy as spy\n",
    "import itertools\n",
    "import copy\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.linalg import expm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HighOrderLaplician(A, Type, Order):\n",
    "    ## Input: \n",
    "    # A is the 1st-order adjacency matrix of the graph\n",
    "\n",
    "    # Type is a str that determines which type of high order Laplician representation to use\n",
    "    # Type='MOL': the function generates the multi-order Laplacian operator, which is \n",
    "    # Type='HOPL': the function generates the high-order path Laplacian operator proposed in our work\n",
    "\n",
    "    # Order is a number that determines which order of interactions to analyze\n",
    "\n",
    "    ## Output:\n",
    "    # L is the high order Laplician representation, which is the Multiorder Laplacian operator or the high-order path Laplacian\n",
    "    Num = A.shape[0]\n",
    "    # Stores the vertices\n",
    "    store = [0]* (Order+1)\n",
    "\n",
    "    # Degree of the vertices\n",
    "    # d = [deg for (_, deg) in G.degree()]\n",
    "    d = list(np.sum(A,axis=0))\n",
    "\n",
    "    Cliques = []\n",
    "    # Function to check if the given set of vertices\n",
    "    # in store array is a clique or not\n",
    "    def is_clique(b) :\n",
    "        # Run a loop for all the set of edges\n",
    "        # for the select vertex\n",
    "        for i in range(b) :\n",
    "            for j in range(i + 1, b) :\n",
    "                # If any edge is missing\n",
    "                if (A[store[i]][store[j]] == 0) :\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    # Function to find all the cliques of size s\n",
    "    def findCliques(i, l, s) :    \n",
    "        # Check if any vertex from i+1 can be inserted as the l-th node in the simplex of size s\n",
    "        for j in range( i + 1, Num) :\n",
    "            # If the degree of the graph is sufficient\n",
    "            if (d[j] >= s - 1) :\n",
    "                # Add the vertex to store\n",
    "                store[l] = j\n",
    "                # If the graph is not a clique of size k\n",
    "                # then it cannot be a clique\n",
    "                # by adding another edge\n",
    "                if (is_clique(l + 1)) :\n",
    "                    # If the length of the clique is\n",
    "                    # still less than the desired size\n",
    "                    if (l < s-1) :\n",
    "                        # Recursion to add vertices\n",
    "                        findCliques(j, l + 1, s)\n",
    "                    # Size is met\n",
    "                    else :\n",
    "                        Cliques.append(store[:s])\n",
    "    findCliques(-1, 0, Order+1)\n",
    "\n",
    "    if len(Cliques)>0:\n",
    "        if Type=='MOL': # the multi-order Laplacian operator\n",
    "            HO_A = np.zeros((Num, Num))\n",
    "            for Simplex in Cliques:\n",
    "                for [i, j] in itertools.combinations(Simplex,2):\n",
    "                    HO_A[i,j] += 1\n",
    "            HO_A += HO_A.T\n",
    "            HO_D = np.zeros(Num)\n",
    "            for Simplex in Cliques:\n",
    "                for [i] in itertools.combinations(Simplex,1):\n",
    "                    HO_D[i] += 1\n",
    "            L = Order * np.diag(HO_D) - HO_A\n",
    "        elif Type=='HOPL': # the high-order path Laplacian operator\n",
    "            HO_B = np.zeros((Num, Num))\n",
    "            for Simplex in Cliques:\n",
    "                for [i, j] in itertools.combinations(Simplex,2):\n",
    "                    HO_B[i,j] += np.math.factorial(Order-1)\n",
    "            HO_B += HO_B.T\n",
    "            HO_P = sum(HO_B)\n",
    "            L = (np.diag(HO_P) - HO_B)/Order\n",
    "    else:\n",
    "        L=np.zeros((Num,Num))\n",
    "    return L\n",
    "\n",
    "def NetworkInfo(L):\n",
    "    ## Input: \n",
    "    # L is the high order Laplician representation, which can be the Multiorder Laplacian operator or the high-order path Laplacian\n",
    "\n",
    "    ## Output:\n",
    "    # Sub_Ls the list of the sub-Laplician-matrices associated with all connected components from L\n",
    "    # Sub_NodeIDs is the list of sub-node-indice associated with all connected components from L\n",
    "    G=nx.from_numpy_array(np.abs(L)-np.diag(np.diag(L)))\n",
    "    Sub_Ls=[L[list(SG),:][:,list(SG)] for SG in nx.connected_components(G)] \n",
    "    Sub_NodeIDs=[list(SG) for SG in nx.connected_components(G)]\n",
    "    return Sub_Ls, Sub_NodeIDs\n",
    "\n",
    "def NetworkUnion(Sub_Ls):\n",
    "    L=np.ones((1,1))\n",
    "    for SL in Sub_Ls:\n",
    "        L=spy.linalg.block_diag(L,SL)\n",
    "    L=L[1:,1:]\n",
    "    return L\n",
    "\n",
    "def SRG_Flow(G,q,p,L_Type,IterNum):\n",
    "    A=nx.adjacency_matrix(G).toarray()       \n",
    "    Lq=HighOrderLaplician(A, L_Type, Order=q)\n",
    "    Lp=HighOrderLaplician(A, L_Type, Order=p)\n",
    "    Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Function(Lq,Lp,q,IterNum)\n",
    "    return  Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment\n",
    "\n",
    "    \n",
    "def SRG_Function(Lq,Lp,q,IterNum):\n",
    "    ## Input: \n",
    "    # Lq is the initial guiding high order Laplician representation, which can be the Multiorder Laplacian operator or the high-order path Laplacian\n",
    "    # Lp is the initial guided high order Laplician representation, which can be the Multiorder Laplacian operator or the high-order path Laplacian\n",
    "\n",
    "    ## Output:\n",
    "    # C_List is the list of specific heat vector calculated by the initial q - order Laplacian L_List[0] and a range of time scale\n",
    "    # Lq_List is the list of the guiding q-order Laplacian over renormalization steps\n",
    "    # Lp_List is the list of the guided p-order Laplacian over renormalization steps\n",
    "    # Gq_List is the list of the guiding q-order network sketches over renormalization steps\n",
    "    # Gp_List is the list of the guided p-order network sketches over renormalization steps\n",
    "    # Tracked_Alignment is the indexes of the initial units aggregated into each macro-unit of all connected clusters after every iteration of the SRG\n",
    "    \n",
    "    Iter=1\n",
    "    Gq=nx.from_numpy_array(np.abs(Lq)-np.diag(np.diag(Lq)))\n",
    "    Gp=nx.from_numpy_array(np.abs(Lp)-np.diag(np.diag(Lp)))\n",
    "    Lq_List=[Lq]\n",
    "    Lp_List=[Lp]\n",
    "    Gq_List=[Gq]\n",
    "    Gp_List=[Gp]\n",
    "    C_List=[]\n",
    "    Init_Sub_Lqs,_=NetworkInfo(Lq)\n",
    "    tau_Vec=np.zeros(len(Init_Sub_Lqs))\n",
    "    for ID in range(len(Init_Sub_Lqs)):\n",
    "        if np.size(Init_Sub_Lqs[ID],0)>1:\n",
    "            tau,CVector=TauSelection(Init_Sub_Lqs[ID])\n",
    "            tau=tau/((q+1)/2)\n",
    "            tau_Vec[ID]=tau\n",
    "            C_List.append(CVector)\n",
    "    All_Alignment=[]\n",
    "    while Iter<IterNum:\n",
    "        ## step 1: initiate Laplacian operators and associated high-order network sketches in the k-th iteration\n",
    "        Origin_Lq,Origin_Lp=Lq_List[Iter-1],Lp_List[Iter-1]\n",
    "        Origin_Gq,Origin_Gp=Gq_List[Iter-1],Gp_List[Iter-1]\n",
    "        Origin_Ap=nx.adjacency_matrix(Origin_Gp).toarray()\n",
    "        New_Ap=copy.deepcopy(Origin_Ap)\n",
    "        New_Lp=copy.deepcopy(Origin_Lp)\n",
    "        Sub_Lqs, Sub_NodeIDs=NetworkInfo(Origin_Lq)\n",
    "        Sub_Gqs=[Origin_Gq.subgraph(SNodeID).copy() for SNodeID in Sub_NodeIDs]\n",
    "        NewSub_Lqs=[]\n",
    "        NewSub_Gqs=[]\n",
    "        New_Snodes_list=[]\n",
    "        ClusterNodeAlignment=[]\n",
    "        for SID in range(len(Sub_Lqs)):\n",
    "            SLq=Sub_Lqs[SID]\n",
    "            SGq=Sub_Gqs[SID]\n",
    "            SNodeID = Sub_NodeIDs[SID]\n",
    "            if np.size(SLq,0)==1:\n",
    "                NodeAlignment=[]\n",
    "                NewSub_Lqs.append(SLq)\n",
    "                NewSub_Gqs.append(SGq)\n",
    "                New_Snodes_list.extend(SNodeID)\n",
    "                NodeAlignment.append([SNodeID[0],SNodeID])\n",
    "            else:\n",
    "                NodeAlignment=[]\n",
    "                ## step 2: search targets to coarse grain in the real space\n",
    "                Rho = expm(-tau*SLq)/np.trace(expm(-tau*SLq))\n",
    "                diagonal_min = np.minimum(Rho.diagonal().reshape(-1, 1), Rho.diagonal())\n",
    "                Rho_prime=Rho/diagonal_min\n",
    "                Rho_prime=(Rho_prime+Rho_prime.T)/2\n",
    "                Ref_adj=(Rho_prime>1).astype('int')\n",
    "                # delete false edges not in subgraph of SLq\n",
    "                Origin_SAq=nx.adjacency_matrix(SGq).toarray()\n",
    "                Ref_mask=(Origin_SAq!=0).astype(int)\n",
    "                Ref_adj=Ref_adj*Ref_mask\n",
    "                RefG = nx.from_numpy_array(Ref_adj) # reference graph\n",
    "                Clusters=[list(c) for c in list(nx.connected_components(RefG))]\n",
    "                n_k=len(Clusters)\n",
    "                ## step 3: implement a renormalization procedure for q-order and p-order network sketches in the real space\n",
    "                # contracting SGq nodes\n",
    "                New_SAq=copy.deepcopy(Origin_SAq)\n",
    "                New_nodes=[Nodes[0] for Nodes in Clusters]\n",
    "                for i in range(n_k):\n",
    "                    iNodes=Clusters[i]\n",
    "                    for j in range(i+1,n_k):\n",
    "                        jNodes=Clusters[j]\n",
    "                        new_weight=sum(Origin_SAq[iNodes,:][:,jNodes].reshape(-1))\n",
    "                        New_SAq[iNodes[0],jNodes[0]]=new_weight\n",
    "                        New_SAq[jNodes[0],iNodes[0]]=new_weight\n",
    "                New_SAq=New_SAq[New_nodes,:][:,New_nodes]\n",
    "                New_SGq=nx.from_numpy_array(New_SAq)\n",
    "                NewSub_Gqs.append(New_SGq)\n",
    "                # contracting Gp nodes\n",
    "                New_Snodes=[SNodeID[Nodes[0]] for Nodes in Clusters]\n",
    "                New_Snodes_list.extend(New_Snodes)\n",
    "                Non_Nodes=[x for x in list(range(New_Ap.shape[0])) if x not in SNodeID]\n",
    "                for i in range(n_k):\n",
    "                    iSNodes = [SNodeID[node] for node in Clusters[i]]\n",
    "                    New_Ap[iSNodes[0],Non_Nodes]=np.sum(Origin_Ap[iSNodes,:][:,Non_Nodes],axis=0) # merge edges from different connected components\n",
    "                    for j in range(i+1,n_k):\n",
    "                        jSNodes = [SNodeID[node] for node in Clusters[j]]\n",
    "                        new_weight=sum(Origin_Ap[iSNodes,:][:,jSNodes].reshape(-1))  \n",
    "                        New_Ap[iSNodes[0],jSNodes[0]]=new_weight\n",
    "                        New_Ap[jSNodes[0],iSNodes[0]]=new_weight\n",
    "                ## step 4: search modes to reduce in the moment space\n",
    "                Evalqs, Evecqs=np.linalg.eig(SLq)\n",
    "                tau=tau_Vec[SID]\n",
    "                Evalq_idx=np.where(np.real(Evalqs)<1/tau)[0]\n",
    "                m_k=len(Evalq_idx) if len(Evalq_idx)>=n_k else n_k\n",
    "                ## step 5: calculate the re-scaled contributions of long-range eigenvectors\n",
    "                SQq = np.zeros((SLq.shape[0],SLq.shape[1]))\n",
    "                Evalq_idx=Evalqs.argsort()[:m_k]\n",
    "                for ID in Evalq_idx:\n",
    "                    Evecq = Evecqs[:,ID].reshape(-1,1)\n",
    "                    SQq += np.real(Evalqs[ID]*np.matmul(Evecq,Evecq.T))\n",
    "\n",
    "                SLp=Origin_Lp[SNodeID,:][:,SNodeID]\n",
    "                Evalps, Evecps = np.linalg.eig(SLp)\n",
    "                Evalp_idx=Evalps.argsort()[:m_k] \n",
    "                SQp = np.zeros((SLp.shape[0],SLp.shape[1]))\n",
    "                for ID in Evalp_idx:\n",
    "                    Evecp = Evecps[:,ID].reshape(-1,1)\n",
    "                    SQp += np.real(Evalps[ID]*np.matmul(Evecp,Evecp.T))\n",
    "                ## step 6: coarse grain q-order and p-order Laplacian in the momentum space \n",
    "                # calculate new SLq and SLp\n",
    "                New_SLq = np.zeros((n_k,n_k))\n",
    "                New_SLp = np.zeros((n_k,n_k))\n",
    "                for i in range(n_k):\n",
    "                    iNodes=Clusters[i]\n",
    "                    iSNodes = [SNodeID[node] for node in Clusters[i]]\n",
    "                    New_Lp[iSNodes[0],Non_Nodes]=np.sum(Origin_Lp[iSNodes,:][:,Non_Nodes],axis=0) # merge edges from different connected components\n",
    "                    for j in range(i,n_k):\n",
    "                        jNodes=Clusters[j]\n",
    "                        new_weight=sum(SQq[iNodes,:][:,jNodes].reshape(-1))\n",
    "                        new_weight0=sum(SQp[iNodes,:][:,jNodes].reshape(-1))\n",
    "                        New_SLq[i,j]=New_SLq[j,i]=new_weight\n",
    "                        New_SLp[i,j]=New_SLp[j,i]=new_weight0\n",
    "                # remove false edges in SLq and SLp\n",
    "                maskq=(New_SAq!=0).astype(int)\n",
    "                New_SLq=New_SLq*maskq\n",
    "                maskp=(New_Ap[New_Snodes,:][:,New_Snodes]).astype(int)\n",
    "                New_SLp=New_SLp*maskp\n",
    "                # ensure the sum of each row of q-order Laplacian operator is zero\n",
    "                for i in range(n_k):\n",
    "                    tmp=list(range(n_k))\n",
    "                    tmp.remove(i)\n",
    "                    New_SLq[i,i]=-np.sum(New_SLq[i,tmp])\n",
    "                NewSub_Lqs.append(New_SLq)\n",
    "\n",
    "                for i in range(n_k):\n",
    "                    iNode1=New_Snodes[i]\n",
    "                    for j in range(i+1,n_k):\n",
    "                        jNode1=New_Snodes[j]\n",
    "                        New_Lp[iNode1,jNode1]=New_SLp[i,j]\n",
    "                        New_Lp[jNode1,iNode1]=New_SLp[j,i]\n",
    "                # track node alignment\n",
    "                for Nodes in Clusters:\n",
    "                    SNodes = [SNodeID[node] for node in Nodes]\n",
    "                    Node1 = SNodes[0]\n",
    "                    NodeAlignment.append([Node1, SNodes])\n",
    "            ClusterNodeAlignment.append(NodeAlignment)\n",
    "        All_Alignment.append(ClusterNodeAlignment)\n",
    "\n",
    "        New_Gq=nx.disjoint_union_all(NewSub_Gqs)\n",
    "        New_Ap=New_Ap[New_Snodes_list,:][:,New_Snodes_list]\n",
    "        New_Ap=np.maximum(New_Ap,New_Ap.T) # gaurantee the symmetry\n",
    "        New_Gp=nx.from_numpy_array(New_Ap)\n",
    "        New_Lp = New_Lp[New_Snodes_list,:][:,New_Snodes_list]\n",
    "        New_Lp=np.minimum(New_Lp,New_Lp.T) # gaurantee the symmetry\n",
    "        # ensure the sum of each row of p-order Laplacian operator is zero\n",
    "        for i in range(New_Lp.shape[0]):\n",
    "            tmp=list(range(New_Lp.shape[0]))\n",
    "            tmp.remove(i)\n",
    "            New_Lp[i,i]=-np.sum(New_Lp[i,tmp])\n",
    "        New_Lq=NetworkUnion(NewSub_Lqs)\n",
    "\n",
    "        Lq_List.append(New_Lq)\n",
    "        Lp_List.append(New_Lp)\n",
    "        Gq_List.append(New_Gq)\n",
    "        Gp_List.append(New_Gp)\n",
    "        Iter=Iter+1\n",
    "    Tracked_Alignment=TrackingUnitID(All_Alignment,Lq.shape[0])\n",
    "    return C_List,Lq_List,Lp_List,Gq_List,Gp_List,All_Alignment\n",
    "\n",
    "def TauSelection(L):\n",
    "    ## Input: \n",
    "    # L is the initial high order Laplician representation, which can be the Multiorder Laplacian operator or the high-order path Laplacian\n",
    "\n",
    "    ## Output:\n",
    "    # tau is the ideal constant that determines the time scale\n",
    "    TauVec=np.logspace(-2, np.log10(50), 200)\n",
    "    A=np.abs(L)-np.diag(np.diag(L))\n",
    "    n_ks=[]\n",
    "    MLambdaVector=np.zeros_like(TauVec)\n",
    "    for ID in range(len(TauVec)):\n",
    "        Poss_tau=TauVec[ID]\n",
    "        MatrixExp=expm(-Poss_tau*L)\n",
    "        Rho=MatrixExp/np.trace(MatrixExp)\n",
    "        MLambdaVector[ID]=np.trace(L @ Rho)\n",
    "\n",
    "        diagonal_min = np.minimum(Rho.diagonal().reshape(-1, 1), Rho.diagonal())\n",
    "        Rho_prime=Rho/diagonal_min\n",
    "        Rho_prime=(Rho_prime+Rho_prime.T)/2\n",
    "        Ref_adj=(Rho_prime>1).astype('int')\n",
    "        # delete false edges not in subgraph of SL\n",
    "        Ref_mask=(A!=0).astype(int)\n",
    "        Ref_adj=Ref_adj*Ref_mask\n",
    "        RefG = nx.from_numpy_array(Ref_adj) # reference graph\n",
    "        n_ks.append(nx.number_connected_components(RefG))\n",
    "    CVector=-np.power(TauVec[1:],2)*np.diff(MLambdaVector)/np.diff(TauVec)\n",
    "    CVector = CVector[np.where(~np.isnan(CVector))[0]]\n",
    "    TauVec = TauVec[np.where(~np.isnan(CVector))[0]]\n",
    "\n",
    "    potential_localmax_id = argrelextrema(CVector,np.greater)[0]\n",
    "    true_localmax_id = potential_localmax_id[np.where(CVector[potential_localmax_id]>0.5*np.max(CVector))[0]]\n",
    "    tau=TauVec[true_localmax_id[0]]\n",
    "\n",
    "    dCVector=np.diff(CVector)/np.diff(TauVec)\n",
    "    plateau_idx = np.where((np.abs(dCVector)<5e-2)&(TauVec[1:]>=tau))[0]\n",
    "    plateau_idx = [i for i in plateau_idx if i+1 in plateau_idx] # find consecutive index as true plateau\n",
    "    if len(plateau_idx)>50 and (CVector[plateau_idx]>0.1*np.max(CVector)).all():\n",
    "        tau = TauVec[plateau_idx[0]]\n",
    "\n",
    "    n_ks=np.array(n_ks)\n",
    "    n_ks=n_ks[np.where(~np.isnan(CVector))[0]]\n",
    "    dn_ks=np.abs(np.diff(n_ks)/np.diff(TauVec))\n",
    "    bi_tau=TauVec[np.argmax(dn_ks)]\n",
    "    tau=tau if tau>bi_tau else bi_tau\n",
    "    print(['our method: tau is ',tau,'!!'])\n",
    "    return tau,CVector\n",
    "\n",
    "def TrackingUnitID(All_Alignment,UnitNum):\n",
    "    # convert nodeID within an iteration (All_Alignment) to global nodeID (Tracked_Alignment)\n",
    "    Tracked_Alignment=copy.deepcopy(All_Alignment)\n",
    "    UnitIDVec=list(range(UnitNum))\n",
    "    for IterID in range(len(All_Alignment)):\n",
    "        if IterID>0:\n",
    "            for ClusterID in range(len(All_Alignment[IterID])):\n",
    "                for CoarseID in range(len(All_Alignment[IterID][ClusterID])):\n",
    "                    NodesToTrack=All_Alignment[IterID][ClusterID][CoarseID][1]\n",
    "                    for IDT in range(len(NodesToTrack)):\n",
    "                        TrackedID=UnitIDVec[NodesToTrack[IDT]]\n",
    "                        Tracked_Alignment[IterID][ClusterID][CoarseID][1][IDT]=TrackedID\n",
    "                    Tracked_Alignment[IterID][ClusterID][CoarseID][0]=Tracked_Alignment[IterID][ClusterID][CoarseID][1][0]\n",
    "        # delete Coarsed node\n",
    "        NodetoDelete=[]\n",
    "        for ClusterID in range(len(Tracked_Alignment[IterID])):\n",
    "            for CoarseID in range(len(Tracked_Alignment[IterID][ClusterID])):\n",
    "                Nodes=Tracked_Alignment[IterID][ClusterID][CoarseID][1][1:]\n",
    "                NodetoDelete.extend(Nodes)\n",
    "\n",
    "        for IDD in NodetoDelete:\n",
    "            UnitIDVec.remove(IDD)\n",
    "\n",
    "    return Tracked_Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.random_graphs.barabasi_albert_graph(1000,3) # Generate a random BA network with 1000 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1144124/835652801.py:100: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A=nx.adjacency_matrix(G).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our method: tau is ', 2.6086304875106583, '!!']\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "[0, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]]\n",
      "[[[0, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]], [43, [43]], [48, [48]], [279, [279]], [281, [281]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1144124/835652801.py:141: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  Origin_Ap=nx.adjacency_matrix(Origin_Gp).toarray()\n",
      "/tmp/ipykernel_1144124/835652801.py:169: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  Origin_SAq=nx.adjacency_matrix(SGq).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our method: tau is ', 2.841764885639019, '!!']\n",
      "['our method: tau is ', 12.710445049387584, '!!']\n",
      "(164, 164)\n",
      "(161, 161)\n",
      "(161, 161)\n",
      "(161, 161)\n",
      "['our method: tau is ', 14.451872596765016, '!!']\n",
      "(300, 300)\n",
      "(299, 299)\n",
      "(298, 298)\n",
      "(298, 298)\n"
     ]
    }
   ],
   "source": [
    "# Multiorder Laplacian operator\n",
    "Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Flow(G,q=1,p=1,L_Type='MOL',IterNum=5) # Run a SRG for 5 iterations, which renormalize the system on the 1-order based on the 1-order interactions\n",
    "\n",
    "Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Flow(G,q=2,p=1,L_Type='MOL',IterNum=5) # Run a SRG for 5 iterations, which renormalize the system on the 1-order based on the 2-order interactions\n",
    "\n",
    "Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Flow(G,q=3,p=1,L_Type='MOL',IterNum=5) # Run a SRG for 5 iterations, which renormalize the system on the 1-order based on the 3-order interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1144124/2469768412.py:100: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A=nx.adjacency_matrix(G).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our method: tau is ', 4.359794641269904, '!!']\n",
      "(36, 36)\n",
      "(28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1144124/2469768412.py:141: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  Origin_Ap=nx.adjacency_matrix(Origin_Gp).toarray()\n",
      "/tmp/ipykernel_1144124/2469768412.py:169: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  Origin_SAq=nx.adjacency_matrix(SGq).toarray()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28)\n",
      "(300, 300)\n",
      "(300, 300)\n",
      "(300, 300)\n",
      "(300, 300)\n",
      "(300, 300)\n",
      "(300, 300)\n",
      "(300, 300)\n",
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "# High-order path Laplacian\n",
    "Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Flow(G,q=1,p=1,L_Type='HOPL',IterNum=5) # Run a SRG for 5 iterations, which renormalize the system on the 1-order based on the 1-order interactions\n",
    "\n",
    "Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Flow(G,q=2,p=1,L_Type='HOPL',IterNum=5) # Run a SRG for 5 iterations, which renormalize the system on the 1-order based on the 2-order interactions\n",
    "\n",
    "Lq_List,Lp_List,Gq_List,Gp_List,C_List,Tracked_Alignment=SRG_Flow(G,q=3,p=1,L_Type='HOPL',IterNum=5) # Run a SRG for 5 iterations, which renormalize the system on the 1-order based on the 3-order interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
